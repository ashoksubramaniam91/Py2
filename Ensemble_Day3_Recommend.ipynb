{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.datasets import load_iris\n",
    "import sklearn.model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "import string\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('C:\\\\Users\\\\Ashok\\\\Desktop\\\\Ensemble Techniques\\\\XGBoost\\\\recommendation system\\\\train.csv')\n",
    "test=pd.read_csv('C:\\\\Users\\\\Ashok\\\\Desktop\\\\Ensemble Techniques\\\\XGBoost\\\\recommendation system\\\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20514, 18)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1         0\n",
       "feature2         0\n",
       "feature3         0\n",
       "feature4         0\n",
       "feature5         0\n",
       "feature6         0\n",
       "feature7         0\n",
       "feature8         0\n",
       "target           0\n",
       "feature1-code    0\n",
       "feature2-code    0\n",
       "feature3-code    0\n",
       "feature4-code    0\n",
       "feature5-code    0\n",
       "feature6-code    0\n",
       "feature7-code    0\n",
       "feature8-code    0\n",
       "target-code      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>target</th>\n",
       "      <th>feature1-code</th>\n",
       "      <th>feature2-code</th>\n",
       "      <th>feature3-code</th>\n",
       "      <th>feature4-code</th>\n",
       "      <th>feature5-code</th>\n",
       "      <th>feature6-code</th>\n",
       "      <th>feature7-code</th>\n",
       "      <th>feature8-code</th>\n",
       "      <th>target-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f1102</td>\n",
       "      <td>f20</td>\n",
       "      <td>f3173</td>\n",
       "      <td>f4619</td>\n",
       "      <td>f5139</td>\n",
       "      <td>f60</td>\n",
       "      <td>item118</td>\n",
       "      <td>item118</td>\n",
       "      <td>item14</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>371</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f1343</td>\n",
       "      <td>f20</td>\n",
       "      <td>f3881</td>\n",
       "      <td>f41745</td>\n",
       "      <td>f5160</td>\n",
       "      <td>f60</td>\n",
       "      <td>item49</td>\n",
       "      <td>item124</td>\n",
       "      <td>item120</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>100</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f120</td>\n",
       "      <td>f20</td>\n",
       "      <td>f324</td>\n",
       "      <td>f41625</td>\n",
       "      <td>f5136</td>\n",
       "      <td>f60</td>\n",
       "      <td>item43</td>\n",
       "      <td>item4</td>\n",
       "      <td>item44</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>97</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f147</td>\n",
       "      <td>f20</td>\n",
       "      <td>f3409</td>\n",
       "      <td>f4877</td>\n",
       "      <td>f5320</td>\n",
       "      <td>f60</td>\n",
       "      <td>item54</td>\n",
       "      <td>item54</td>\n",
       "      <td>item54</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>131</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f197</td>\n",
       "      <td>f20</td>\n",
       "      <td>f3257</td>\n",
       "      <td>f4299</td>\n",
       "      <td>f5208</td>\n",
       "      <td>f60</td>\n",
       "      <td>item47</td>\n",
       "      <td>item47</td>\n",
       "      <td>item126</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>304</td>\n",
       "      <td>409</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature1 feature2 feature3 feature4 feature5 feature6 feature7 feature8  \\\n",
       "0    f1102      f20    f3173    f4619    f5139      f60  item118  item118   \n",
       "1    f1343      f20    f3881   f41745    f5160      f60   item49  item124   \n",
       "2     f120      f20     f324   f41625    f5136      f60   item43    item4   \n",
       "3     f147      f20    f3409    f4877    f5320      f60   item54   item54   \n",
       "4     f197      f20    f3257    f4299    f5208      f60   item47   item47   \n",
       "\n",
       "    target  feature1-code  feature2-code  feature3-code  feature4-code  \\\n",
       "0   item14             92              1            144            371   \n",
       "1  item120              5              1              7              7   \n",
       "2   item44              4              1              4             97   \n",
       "3   item54             56              1             81            131   \n",
       "4  item126             10              1            304            409   \n",
       "\n",
       "   feature5-code  feature6-code  feature7-code  feature8-code  target-code  \n",
       "0            128              1             11             11           49  \n",
       "1              7              1             45            100          142  \n",
       "2             78              1              2             82           64  \n",
       "3             71              1             46             46           46  \n",
       "4            129              1              3              3           68  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 49, 142,  64,  46,  68, 129, 158,  21,  22,  60,   8,  42,  30,\n",
       "        69,  24,  27,  23,  81, 166,  51,  83,  16,  54,  29,  31,  38,\n",
       "        12,  66,  86,  45,  70,  96,   6,  35, 106, 105, 117,  72,  44,\n",
       "        53,  10, 113,  41,   5,  17,  93,  59,  33,  47, 108,  20,  88,\n",
       "        37,  82,  15,  95,  11, 125, 162,  63,   1,   4,  55, 164, 167,\n",
       "        19, 104, 109,  94,  75,  28,   3,  92,  84,  18,  77,  56, 144,\n",
       "       143,  13, 107,  57,  34, 170,  91,  48,  39,  14,  58,  50, 114,\n",
       "        25, 122,  90, 101, 132, 126, 151, 131, 130,  78, 139, 110,  76,\n",
       "       168,   9,  99,  62, 100, 120, 152, 155, 135,  32, 128, 146, 163,\n",
       "         7, 112,  43, 133,  26,  98,   2, 102, 137, 141, 127, 134,  89,\n",
       "       154,  74,  65,  67,  79,  71, 123, 138, 111,  52, 153,  73, 148,\n",
       "       116, 115,  87,  40, 140, 103, 161,  36, 165,  97, 159,  80, 149,\n",
       "       147, 121, 124, 160, 150, 118, 145, 157, 136, 156,  61, 119,  85,\n",
       "       173, 172, 171, 169], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target-code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurecolumns = ['feature1-code',\n",
    "              'feature2-code',\n",
    "              'feature3-code',\n",
    "              'feature4-code',\n",
    "              'feature5-code',\n",
    "              'feature6-code',\n",
    "              'feature7-code',\n",
    "              'feature8-code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[featurecolumns].values\n",
    "Y_train = train['target-code'].values\n",
    "X_test = test[featurecolumns].values\n",
    "Y_test = test['target-code'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashok\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1604601286800546\n",
      "[[13  0  3 ...  0  0  0]\n",
      " [ 1  2  0 ...  0  0  0]\n",
      " [ 1  0  5 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.15      0.18      0.16        74\n",
      "           2       0.07      0.06      0.07        31\n",
      "           3       0.06      0.07      0.06        69\n",
      "           4       0.23      0.27      0.25        73\n",
      "           5       0.24      0.29      0.26       124\n",
      "           6       0.07      0.11      0.08        76\n",
      "           7       0.09      0.10      0.09        31\n",
      "           8       0.14      0.21      0.17       156\n",
      "           9       0.15      0.12      0.13        25\n",
      "          10       0.11      0.14      0.12        36\n",
      "          11       0.21      0.22      0.21        37\n",
      "          12       0.18      0.22      0.20       249\n",
      "          13       0.24      0.32      0.28       140\n",
      "          14       0.12      0.12      0.12        16\n",
      "          15       0.41      0.42      0.41        96\n",
      "          16       0.22      0.25      0.23       120\n",
      "          17       0.16      0.14      0.15        36\n",
      "          18       0.13      0.15      0.14       110\n",
      "          19       0.22      0.23      0.23       155\n",
      "          20       0.21      0.23      0.22        81\n",
      "          21       0.16      0.13      0.14        38\n",
      "          22       0.23      0.26      0.25       165\n",
      "          23       0.16      0.17      0.17       127\n",
      "          24       0.26      0.23      0.24        22\n",
      "          25       0.20      0.08      0.11        13\n",
      "          26       0.05      0.03      0.04        32\n",
      "          27       0.15      0.13      0.14        60\n",
      "          28       0.12      0.14      0.13        22\n",
      "          29       0.05      0.05      0.05        43\n",
      "          30       0.23      0.21      0.22        58\n",
      "          31       0.10      0.11      0.10        37\n",
      "          32       0.08      0.08      0.08        13\n",
      "          33       0.21      0.19      0.20        59\n",
      "          34       0.14      0.09      0.11        22\n",
      "          35       0.20      0.12      0.15         8\n",
      "          36       0.14      0.19      0.16        16\n",
      "          37       0.17      0.14      0.16        49\n",
      "          38       0.00      0.00      0.00        13\n",
      "          39       0.12      0.16      0.14        19\n",
      "          40       0.00      0.00      0.00         8\n",
      "          41       0.15      0.19      0.17        72\n",
      "          42       0.06      0.03      0.04        31\n",
      "          43       0.00      0.00      0.00         2\n",
      "          44       0.09      0.08      0.09        24\n",
      "          45       0.00      0.00      0.00         7\n",
      "          46       0.12      0.16      0.14        32\n",
      "          47       0.11      0.09      0.10        55\n",
      "          48       0.00      0.00      0.00        10\n",
      "          49       0.21      0.19      0.20        26\n",
      "          50       0.08      0.07      0.08       107\n",
      "          51       0.12      0.09      0.11        11\n",
      "          52       0.00      0.00      0.00         9\n",
      "          53       0.12      0.10      0.11        83\n",
      "          54       0.07      0.05      0.06        20\n",
      "          55       0.11      0.08      0.09        50\n",
      "          56       0.08      0.07      0.07        59\n",
      "          57       0.00      0.00      0.00        28\n",
      "          58       0.07      0.06      0.07        47\n",
      "          59       0.32      0.37      0.34       215\n",
      "          60       0.00      0.00      0.00        12\n",
      "          61       0.00      0.00      0.00         8\n",
      "          62       0.00      0.00      0.00        10\n",
      "          63       0.00      0.00      0.00        10\n",
      "          64       0.14      0.10      0.12        20\n",
      "          65       0.20      0.17      0.18         6\n",
      "          66       0.05      0.03      0.04        31\n",
      "          67       0.00      0.00      0.00        13\n",
      "          68       0.00      0.00      0.00         5\n",
      "          69       0.07      0.06      0.06        16\n",
      "          70       0.10      0.09      0.09        67\n",
      "          71       0.14      0.20      0.17         5\n",
      "          72       0.07      0.06      0.07        64\n",
      "          73       0.00      0.00      0.00        10\n",
      "          74       0.00      0.00      0.00        12\n",
      "          75       0.22      0.21      0.22        47\n",
      "          76       0.24      0.17      0.20        24\n",
      "          77       0.18      0.15      0.17        39\n",
      "          78       0.09      0.09      0.09        32\n",
      "          79       0.33      0.08      0.13        12\n",
      "          80       0.00      0.00      0.00         9\n",
      "          81       0.00      0.00      0.00        28\n",
      "          82       0.16      0.14      0.15        69\n",
      "          83       0.16      0.18      0.17        56\n",
      "          84       0.07      0.07      0.07        46\n",
      "          85       0.33      0.50      0.40         2\n",
      "          86       0.15      0.13      0.14        15\n",
      "          87       0.12      0.12      0.12         8\n",
      "          88       0.06      0.03      0.04        30\n",
      "          89       0.11      0.07      0.08        15\n",
      "          90       0.29      0.10      0.15        20\n",
      "          91       0.00      0.00      0.00        12\n",
      "          92       0.00      0.00      0.00         6\n",
      "          93       0.06      0.05      0.05        21\n",
      "          94       0.10      0.06      0.08        16\n",
      "          95       0.33      0.07      0.11        15\n",
      "          96       0.17      0.19      0.18        63\n",
      "          97       0.14      0.11      0.12         9\n",
      "          98       0.00      0.00      0.00         2\n",
      "          99       0.00      0.00      0.00         7\n",
      "         100       0.14      0.11      0.12        28\n",
      "         101       0.19      0.14      0.16        37\n",
      "         102       0.00      0.00      0.00         7\n",
      "         103       0.13      0.13      0.13        15\n",
      "         104       0.00      0.00      0.00         4\n",
      "         105       0.24      0.30      0.26        27\n",
      "         106       0.00      0.00      0.00        38\n",
      "         107       0.05      0.04      0.05        23\n",
      "         108       0.27      0.18      0.21        17\n",
      "         109       0.00      0.00      0.00         4\n",
      "         110       0.00      0.00      0.00        13\n",
      "         111       0.25      0.12      0.17         8\n",
      "         112       0.14      0.09      0.11        11\n",
      "         113       0.19      0.21      0.20        19\n",
      "         114       0.00      0.00      0.00        10\n",
      "         115       0.24      0.16      0.19        31\n",
      "         116       0.11      0.08      0.10        12\n",
      "         117       0.20      0.11      0.14        18\n",
      "         118       0.00      0.00      0.00         1\n",
      "         119       0.00      0.00      0.00         3\n",
      "         120       0.09      0.08      0.08        26\n",
      "         121       0.17      0.17      0.17         6\n",
      "         122       0.14      0.12      0.13        33\n",
      "         123       0.00      0.00      0.00         6\n",
      "         124       0.25      0.25      0.25         4\n",
      "         125       0.11      0.08      0.09        24\n",
      "         126       0.20      0.08      0.12        12\n",
      "         127       0.00      0.00      0.00        14\n",
      "         128       0.15      0.13      0.14        15\n",
      "         129       0.11      0.08      0.10        12\n",
      "         130       0.10      0.12      0.11         8\n",
      "         131       0.10      0.07      0.08        14\n",
      "         132       0.08      0.04      0.06        24\n",
      "         133       0.17      0.20      0.18        10\n",
      "         134       0.00      0.00      0.00        18\n",
      "         135       0.00      0.00      0.00         5\n",
      "         136       0.20      0.14      0.17         7\n",
      "         137       0.06      0.06      0.06        16\n",
      "         138       0.22      0.22      0.22         9\n",
      "         139       0.12      0.09      0.11        11\n",
      "         140       0.17      0.20      0.18         5\n",
      "         141       0.00      0.00      0.00         3\n",
      "         142       0.00      0.00      0.00         9\n",
      "         143       0.12      0.08      0.10        12\n",
      "         144       0.00      0.00      0.00        10\n",
      "         145       0.17      0.17      0.17         6\n",
      "         146       0.11      0.14      0.12        14\n",
      "         147       0.00      0.00      0.00         3\n",
      "         148       0.00      0.00      0.00         6\n",
      "         149       0.11      0.20      0.14         5\n",
      "         150       0.33      0.08      0.13        12\n",
      "         151       0.20      0.17      0.18         6\n",
      "         152       0.17      0.14      0.15         7\n",
      "         153       0.15      0.18      0.17        11\n",
      "         154       0.00      0.00      0.00         5\n",
      "         155       0.33      0.20      0.25         5\n",
      "         156       0.00      0.00      0.00         2\n",
      "         157       0.00      0.00      0.00         4\n",
      "         158       0.00      0.00      0.00         5\n",
      "         159       0.00      0.00      0.00         3\n",
      "         160       0.00      0.00      0.00         8\n",
      "         161       0.00      0.00      0.00         1\n",
      "         162       0.14      0.10      0.12        10\n",
      "         163       0.06      0.10      0.08        10\n",
      "         164       0.33      0.25      0.29         4\n",
      "         165       0.12      0.25      0.17         4\n",
      "         166       0.00      0.00      0.00         5\n",
      "         167       0.00      0.00      0.00         4\n",
      "         168       0.00      0.00      0.00         1\n",
      "         169       0.50      1.00      0.67         1\n",
      "         170       0.00      0.00      0.00         4\n",
      "         171       0.00      0.00      0.00         2\n",
      "         172       0.00      0.00      0.00         2\n",
      "         173       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.16      0.16      0.16      5129\n",
      "   macro avg       0.11      0.10      0.10      5129\n",
      "weighted avg       0.15      0.16      0.16      5129\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashok\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07876779099239618\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        74\n",
      "           2       0.00      0.00      0.00        31\n",
      "           3       0.00      0.00      0.00        69\n",
      "           4       0.00      0.00      0.00        73\n",
      "           5       0.00      0.00      0.00       124\n",
      "           6       0.00      0.00      0.00        76\n",
      "           7       0.00      0.00      0.00        31\n",
      "           8       0.09      0.38      0.14       156\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        36\n",
      "          11       0.00      0.00      0.00        37\n",
      "          12       0.07      0.48      0.12       249\n",
      "          13       0.00      0.00      0.00       140\n",
      "          14       0.00      0.00      0.00        16\n",
      "          15       0.00      0.00      0.00        96\n",
      "          16       0.00      0.00      0.00       120\n",
      "          17       0.00      0.00      0.00        36\n",
      "          18       0.00      0.00      0.00       110\n",
      "          19       0.11      0.30      0.16       155\n",
      "          20       0.00      0.00      0.00        81\n",
      "          21       0.00      0.00      0.00        38\n",
      "          22       0.00      0.00      0.00       165\n",
      "          23       0.07      0.37      0.11       127\n",
      "          24       0.00      0.00      0.00        22\n",
      "          25       0.00      0.00      0.00        13\n",
      "          26       0.00      0.00      0.00        32\n",
      "          27       0.00      0.00      0.00        60\n",
      "          28       0.00      0.00      0.00        22\n",
      "          29       0.00      0.00      0.00        43\n",
      "          30       0.00      0.00      0.00        58\n",
      "          31       0.00      0.00      0.00        37\n",
      "          32       0.25      0.08      0.12        13\n",
      "          33       0.00      0.00      0.00        59\n",
      "          34       0.00      0.00      0.00        22\n",
      "          35       0.00      0.00      0.00         8\n",
      "          36       0.00      0.00      0.00        16\n",
      "          37       0.00      0.00      0.00        49\n",
      "          38       0.00      0.00      0.00        13\n",
      "          39       0.00      0.00      0.00        19\n",
      "          40       0.00      0.00      0.00         8\n",
      "          41       0.10      0.04      0.06        72\n",
      "          42       0.00      0.00      0.00        31\n",
      "          43       0.00      0.00      0.00         2\n",
      "          44       0.01      0.04      0.02        24\n",
      "          45       0.00      0.00      0.00         7\n",
      "          46       0.00      0.00      0.00        32\n",
      "          47       0.00      0.00      0.00        55\n",
      "          48       0.00      0.00      0.00        10\n",
      "          49       0.00      0.00      0.00        26\n",
      "          50       0.00      0.00      0.00       107\n",
      "          51       0.00      0.00      0.00        11\n",
      "          52       0.00      0.00      0.00         9\n",
      "          53       0.00      0.00      0.00        83\n",
      "          54       0.00      0.00      0.00        20\n",
      "          55       0.00      0.00      0.00        50\n",
      "          56       0.00      0.00      0.00        59\n",
      "          57       0.00      0.00      0.00        28\n",
      "          58       0.00      0.00      0.00        47\n",
      "          59       0.14      0.56      0.22       215\n",
      "          60       0.00      0.00      0.00        12\n",
      "          61       0.00      0.00      0.00         8\n",
      "          62       0.00      0.00      0.00        10\n",
      "          63       0.00      0.00      0.00        10\n",
      "          64       0.00      0.00      0.00        20\n",
      "          65       0.00      0.00      0.00         6\n",
      "          66       0.00      0.00      0.00        31\n",
      "          67       0.00      0.00      0.00        13\n",
      "          68       0.00      0.00      0.00         5\n",
      "          69       0.00      0.00      0.00        16\n",
      "          70       0.33      0.01      0.03        67\n",
      "          71       0.00      0.00      0.00         5\n",
      "          72       0.00      0.00      0.00        64\n",
      "          73       0.00      0.00      0.00        10\n",
      "          74       0.00      0.00      0.00        12\n",
      "          75       0.00      0.00      0.00        47\n",
      "          76       0.00      0.00      0.00        24\n",
      "          77       0.00      0.00      0.00        39\n",
      "          78       0.00      0.00      0.00        32\n",
      "          79       0.00      0.00      0.00        12\n",
      "          80       0.00      0.00      0.00         9\n",
      "          81       0.00      0.00      0.00        28\n",
      "          82       0.00      0.00      0.00        69\n",
      "          83       0.00      0.00      0.00        56\n",
      "          84       0.00      0.00      0.00        46\n",
      "          85       0.00      0.00      0.00         2\n",
      "          86       0.00      0.00      0.00        15\n",
      "          87       0.00      0.00      0.00         8\n",
      "          88       0.00      0.00      0.00        30\n",
      "          89       0.00      0.00      0.00        15\n",
      "          90       0.00      0.00      0.00        20\n",
      "          91       0.00      0.00      0.00        12\n",
      "          92       0.00      0.00      0.00         6\n",
      "          93       0.00      0.00      0.00        21\n",
      "          94       0.00      0.00      0.00        16\n",
      "          95       0.00      0.00      0.00        15\n",
      "          96       0.00      0.00      0.00        63\n",
      "          97       0.00      0.00      0.00         9\n",
      "          98       0.00      0.00      0.00         2\n",
      "          99       0.00      0.00      0.00         7\n",
      "         100       0.00      0.00      0.00        28\n",
      "         101       0.00      0.00      0.00        37\n",
      "         102       0.00      0.00      0.00         7\n",
      "         103       0.00      0.00      0.00        15\n",
      "         104       0.00      0.00      0.00         4\n",
      "         105       0.00      0.00      0.00        27\n",
      "         106       0.00      0.00      0.00        38\n",
      "         107       0.00      0.00      0.00        23\n",
      "         108       0.00      0.00      0.00        17\n",
      "         109       0.00      0.00      0.00         4\n",
      "         110       0.00      0.00      0.00        13\n",
      "         111       0.00      0.00      0.00         8\n",
      "         112       0.00      0.00      0.00        11\n",
      "         113       0.00      0.00      0.00        19\n",
      "         114       0.00      0.00      0.00        10\n",
      "         115       0.00      0.00      0.00        31\n",
      "         116       0.00      0.00      0.00        12\n",
      "         117       0.00      0.00      0.00        18\n",
      "         118       0.00      0.00      0.00         1\n",
      "         119       0.00      0.00      0.00         3\n",
      "         120       0.00      0.00      0.00        26\n",
      "         121       0.00      0.00      0.00         6\n",
      "         122       0.00      0.00      0.00        33\n",
      "         123       0.00      0.00      0.00         6\n",
      "         124       0.00      0.00      0.00         4\n",
      "         125       0.00      0.00      0.00        24\n",
      "         126       0.00      0.00      0.00        12\n",
      "         127       0.00      0.00      0.00        14\n",
      "         128       0.00      0.00      0.00        15\n",
      "         129       0.00      0.00      0.00        12\n",
      "         130       0.00      0.00      0.00         8\n",
      "         131       0.00      0.00      0.00        14\n",
      "         132       0.00      0.00      0.00        24\n",
      "         133       0.02      0.30      0.03        10\n",
      "         134       0.00      0.00      0.00        18\n",
      "         135       0.00      0.00      0.00         5\n",
      "         136       0.00      0.00      0.00         7\n",
      "         137       0.00      0.00      0.00        16\n",
      "         138       0.00      0.00      0.00         9\n",
      "         139       0.00      0.00      0.00        11\n",
      "         140       0.00      0.00      0.00         5\n",
      "         141       0.00      0.00      0.00         3\n",
      "         142       0.00      0.00      0.00         9\n",
      "         143       0.00      0.00      0.00        12\n",
      "         144       0.00      0.00      0.00        10\n",
      "         145       0.00      0.00      0.00         6\n",
      "         146       0.00      0.00      0.00        14\n",
      "         147       0.00      0.00      0.00         3\n",
      "         148       0.00      0.00      0.00         6\n",
      "         149       0.00      0.00      0.00         5\n",
      "         150       0.00      0.00      0.00        12\n",
      "         151       0.00      0.00      0.00         6\n",
      "         152       0.00      0.00      0.00         7\n",
      "         153       0.00      0.00      0.00        11\n",
      "         154       0.00      0.00      0.00         5\n",
      "         155       0.00      0.00      0.00         5\n",
      "         156       0.00      0.00      0.00         2\n",
      "         157       0.00      0.00      0.00         4\n",
      "         158       0.00      0.00      0.00         5\n",
      "         159       0.00      0.00      0.00         3\n",
      "         160       0.00      0.00      0.00         8\n",
      "         161       0.00      0.00      0.00         1\n",
      "         162       0.00      0.00      0.00        10\n",
      "         163       0.00      0.00      0.00        10\n",
      "         164       0.00      0.00      0.00         4\n",
      "         165       0.00      0.00      0.00         4\n",
      "         166       0.00      0.00      0.00         5\n",
      "         167       0.00      0.00      0.00         4\n",
      "         168       0.00      0.00      0.00         1\n",
      "         169       0.00      0.00      0.00         1\n",
      "         170       0.00      0.00      0.00         4\n",
      "         171       0.00      0.00      0.00         2\n",
      "         172       0.00      0.00      0.00         2\n",
      "         173       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.08      0.08      0.08      5129\n",
      "   macro avg       0.01      0.01      0.01      5129\n",
      "weighted avg       0.02      0.08      0.03      5129\n",
      "\n",
      "\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashok\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier()\n",
    "Ada=AdaBoostClassifier()\n",
    "models = [rf,Ada]\n",
    "for mod in models : \n",
    "    print(mod)\n",
    "    print(\"\\n\")\n",
    "    mod.fit(X_train,Y_train)\n",
    "    ypred = mod.predict(X_test)\n",
    "    print(accuracy_score(Y_test,ypred))\n",
    "    conf_matrix = confusion_matrix(Y_test, ypred)\n",
    "    print(conf_matrix)\n",
    "    print(classification_report(Y_test, ypred))\n",
    "    print(\"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
