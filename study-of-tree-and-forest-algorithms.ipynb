{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import random_integers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from scipy.stats import pointbiserialr, spearmanr\n",
    "%matplotlib inline\n",
    "\n",
    "print('Libraries Ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training data\n",
    "\n",
    "path = '../input/'\n",
    "df = pd.read_csv(path+'train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Process Data\n",
    "\n",
    "People with stronger titles tend to have more help on board. Hence, we will categorize passengers based on titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "                    } \n",
    "\n",
    "df['Title'] = df['Name'].apply(lambda x: Title_Dictionary[x.split(',')[1].split('.')[0].strip()])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ticket prefix may determine the status or cabin on board and hence will be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ticket_Prefix(s):\n",
    "    s=s.split()[0]\n",
    "    if s.isdigit():\n",
    "        return 'NoClue'\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "df['TicketPrefix'] = df['Ticket'].apply(lambda x: Ticket_Prefix(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check for data types and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Age and Embarked has missing data.\n",
    "\n",
    "Simply dropping the Age NaNs would mean throwing away too much data.\n",
    "\n",
    "We add in the median age based on the Title, Pclass and Sex of each passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_Age = df.Age.notnull()\n",
    "Age_Sex_Title_Pclass = df.loc[mask_Age, [\"Age\", \"Title\", \"Sex\", \"Pclass\"]]\n",
    "Filler_Ages = Age_Sex_Title_Pclass.groupby(by = [\"Title\", \"Pclass\", \"Sex\"]).median()\n",
    "Filler_Ages = Filler_Ages.Age.unstack(level = -1).unstack(level = -1)\n",
    "\n",
    "mask_Age = df.Age.isnull()\n",
    "Age_Sex_Title_Pclass_missing = df.loc[mask_Age, [\"Title\", \"Sex\", \"Pclass\"]]\n",
    "\n",
    "def Age_filler(row):\n",
    "    if row.Sex == \"female\":\n",
    "        age = Filler_Ages.female.loc[row[\"Title\"], row[\"Pclass\"]]\n",
    "        return age\n",
    "    \n",
    "    elif row.Sex == \"male\":\n",
    "        age = Filler_Ages.male.loc[row[\"Title\"], row[\"Pclass\"]]\n",
    "        return age\n",
    "    \n",
    "Age_Sex_Title_Pclass_missing[\"Age\"]  = Age_Sex_Title_Pclass_missing.apply(Age_filler, axis = 1)   \n",
    "\n",
    "df[\"Age\"] = pd.concat([Age_Sex_Title_Pclass[\"Age\"], Age_Sex_Title_Pclass_missing[\"Age\"]])    \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fill in the missing Fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fare']=df['Fare'].fillna(value=df.Fare.mean())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need Cabin and Ticket and hence can be dropped from our DataFrame.\n",
    "\n",
    "We also can combine SibSp and Parch to FamilySize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FamilySize'] = df['SibSp'] + df['Parch']\n",
    "df = df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we deal with categorical data using dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_Sex=pd.get_dummies(df['Sex'],prefix='Sex')\n",
    "dummies_Embarked = pd.get_dummies(df['Embarked'], prefix= 'Embarked') \n",
    "dummies_Pclass = pd.get_dummies(df['Pclass'], prefix= 'Pclass')\n",
    "dummies_Title = pd.get_dummies(df['Title'], prefix= 'Title')\n",
    "dummies_TicketPrefix = pd.get_dummies(df['TicketPrefix'], prefix='TicketPrefix')\n",
    "df = pd.concat([df, dummies_Sex, dummies_Embarked, dummies_Pclass, dummies_Title, dummies_TicketPrefix], axis=1)\n",
    "df = df.drop(['Sex','Embarked','Pclass','Title','Name','TicketPrefix'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set our PassengerId as our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(['PassengerId'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Feature Selection\n",
    "\n",
    "For feature selection, we will look at the correlation of each feature against Survived.\n",
    "Based on our data types, we will use the following aglorithms:\n",
    "\n",
    "- Spearman-Rank correlation for nominal vs nominal data\n",
    "- Point-Biserial correlation for nominal vs continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns.values\n",
    "\n",
    "param=[]\n",
    "correlation=[]\n",
    "abs_corr=[]\n",
    "\n",
    "for c in columns:\n",
    "    #Check if binary or continuous\n",
    "    if len(df[c].unique())<=2:\n",
    "        corr = spearmanr(df['Survived'],df[c])[0]\n",
    "    else:\n",
    "        corr = pointbiserialr(df['Survived'],df[c])[0]\n",
    "    param.append(c)\n",
    "    correlation.append(corr)\n",
    "    abs_corr.append(abs(corr))\n",
    "\n",
    "#Create dataframe for visualization\n",
    "param_df=pd.DataFrame({'correlation':correlation,'parameter':param, 'abs_corr':abs_corr})\n",
    "\n",
    "#Sort by absolute correlation\n",
    "param_df=param_df.sort_values(by=['abs_corr'], ascending=False)\n",
    "\n",
    "#Set parameter name as index\n",
    "param_df=param_df.set_index('parameter')\n",
    "\n",
    "param_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our correlation, we can use the Decision Tree classifier to see the score agaisnt feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresCV = []\n",
    "scores = []\n",
    "\n",
    "for i in range(1,len(param_df)):\n",
    "    new_df=df[param_df.index[0:i+1].values]\n",
    "    X = new_df.ix[:,1::]\n",
    "    y = new_df.ix[:,0]\n",
    "    clf = DecisionTreeClassifier()\n",
    "    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n",
    "    scores.append(np.mean(scoreCV))\n",
    "    \n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(1,len(scores)+1),scores, '.-')\n",
    "plt.axis(\"tight\")\n",
    "plt.title('Feature Selection', fontsize=14)\n",
    "plt.xlabel('# Features', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot, a feature space of 10 dimensions provides the most reliable result while avoiding overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features=param_df.index[1:10+1].values\n",
    "print('Best features:\\t',best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at out best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[best_features].hist(figsize=(20,15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[best_features]\n",
    "y = df['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Decision Tree\n",
    "\n",
    "Analyzing the different parameters of Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "#Max Features\n",
    "plt.subplot(2,3,1)\n",
    "feature_param = ['auto','sqrt','log2',None]\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = DecisionTreeClassifier(max_features=feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n",
    "    scores.append(np.mean(scoreCV))\n",
    "plt.plot(scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Max Features')\n",
    "plt.xticks(range(len(feature_param)), feature_param)\n",
    "plt.grid();\n",
    "\n",
    "#Max Depth\n",
    "plt.subplot(2,3,2)\n",
    "feature_param = range(1,51)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = DecisionTreeClassifier(max_depth=feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n",
    "    scores.append(np.mean(scoreCV))\n",
    "plt.plot(feature_param, scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Max Depth')\n",
    "plt.grid();\n",
    "\n",
    "#Min Samples Split\n",
    "plt.subplot(2,3,3)\n",
    "feature_param = range(1,51)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = DecisionTreeClassifier(min_samples_split =feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n",
    "    scores.append(np.mean(scoreCV))\n",
    "plt.plot(feature_param, scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Min Samples Split')\n",
    "plt.grid();\n",
    "\n",
    "#Min Samples Leaf\n",
    "plt.subplot(2,3,4)\n",
    "feature_param = range(1,51)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf =feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n",
    "    scores.append(np.mean(scoreCV))\n",
    "plt.plot(feature_param, scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Min Samples Leaf')\n",
    "plt.grid();\n",
    "\n",
    "#Min Weight Fraction Leaf\n",
    "plt.subplot(2,3,5)\n",
    "feature_param = np.linspace(0,0.5,10)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = DecisionTreeClassifier(min_weight_fraction_leaf =feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n",
    "    scores.append(np.mean(scoreCV))\n",
    "plt.plot(feature_param, scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Min Weight Fraction Leaf')\n",
    "plt.grid();\n",
    "\n",
    "#Max Leaf Nodes\n",
    "plt.subplot(2,3,6)\n",
    "feature_param = range(2,21)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = DecisionTreeClassifier(max_leaf_nodes=feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n",
    "    scores.append(np.mean(scoreCV))\n",
    "plt.plot(feature_param, scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Max Leaf Nodes')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Max Depth show high score variance with change in parameter.\n",
    "- All otehr parameters show low score variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "#N Estimators\n",
    "plt.subplot(3,3,1)\n",
    "feature_param = range(1,21)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = RandomForestClassifier(n_estimators=feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('N Estimators')\n",
    "plt.grid();\n",
    "\n",
    "#Criterion\n",
    "plt.subplot(3,3,2)\n",
    "feature_param = ['gini','entropy']\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = RandomForestClassifier(criterion=feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(scores, '.-')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Criterion')\n",
    "plt.xticks(range(len(feature_param)), feature_param)\n",
    "plt.grid();\n",
    "\n",
    "#Max Features\n",
    "plt.subplot(3,3,3)\n",
    "feature_param = ['auto','sqrt','log2',None]\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = RandomForestClassifier(max_features=feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Max Features')\n",
    "plt.xticks(range(len(feature_param)), feature_param)\n",
    "plt.grid();\n",
    "\n",
    "#Max Depth\n",
    "plt.subplot(3,3,4)\n",
    "feature_param = range(1,21)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = RandomForestClassifier(max_depth=feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(feature_param, scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Max Depth')\n",
    "plt.grid();\n",
    "\n",
    "#Min Samples Split\n",
    "plt.subplot(3,3,5)\n",
    "feature_param = range(1,21)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = RandomForestClassifier(min_samples_split =feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(feature_param, scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Min Samples Split')\n",
    "plt.grid();\n",
    "\n",
    "#Min Weight Fraction Leaf\n",
    "plt.subplot(3,3,6)\n",
    "feature_param = np.linspace(0,0.5,10)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = RandomForestClassifier(min_weight_fraction_leaf =feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(feature_param, scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Min Weight Fraction Leaf')\n",
    "plt.grid();\n",
    "\n",
    "#Max Leaf Nodes\n",
    "plt.subplot(3,3,7)\n",
    "feature_param = range(2,21)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = RandomForestClassifier(max_leaf_nodes=feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(feature_param, scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Max Leaf Nodes')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest seems to show high variance in scores with most parameter changes.\n",
    "- Max Features, Criterion and Max Leaf Nodes show low variance in scores.\n",
    "- The general high varience in N Estimator plot shows the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "#N Estimators\n",
    "plt.subplot(3,3,1)\n",
    "feature_param = range(1,21)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = GradientBoostingClassifier(n_estimators=feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('N Estimators')\n",
    "plt.grid();\n",
    "\n",
    "#Learning Rate\n",
    "plt.subplot(3,3,2)\n",
    "feature_param = np.linspace(0.1,1,10)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = GradientBoostingClassifier(learning_rate=feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(scores, '.-')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Learning Rate')\n",
    "plt.grid();\n",
    "\n",
    "#Max Features\n",
    "plt.subplot(3,3,3)\n",
    "feature_param = ['auto','sqrt','log2',None]\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = GradientBoostingClassifier(max_features=feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Max Features')\n",
    "plt.grid();\n",
    "\n",
    "#Max Depth\n",
    "plt.subplot(3,3,4)\n",
    "feature_param = range(1,21)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = GradientBoostingClassifier(max_depth=feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(feature_param, scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Max Depth')\n",
    "plt.grid();\n",
    "\n",
    "#Min Samples Split\n",
    "plt.subplot(3,3,5)\n",
    "feature_param = range(1,21)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = GradientBoostingClassifier(min_samples_split =feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(feature_param, scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Min Samples Split')\n",
    "plt.grid();\n",
    "\n",
    "#Min Weight Fraction Leaf\n",
    "plt.subplot(3,3,6)\n",
    "feature_param = np.linspace(0,0.5,10)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = GradientBoostingClassifier(min_weight_fraction_leaf =feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(feature_param, scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Min Weight Fraction Leaf')\n",
    "plt.grid();\n",
    "\n",
    "#Max Leaf Nodes\n",
    "plt.subplot(3,3,7)\n",
    "feature_param = range(2,21)\n",
    "scores=[]\n",
    "for feature in feature_param:\n",
    "    clf = GradientBoostingClassifier(max_leaf_nodes=feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    scoreCV = clf.score(X_test,y_test)\n",
    "    scores.append(scoreCV)\n",
    "plt.plot(feature_param, scores, '.-')\n",
    "plt.axis('tight')\n",
    "# plt.xlabel('parameter')\n",
    "# plt.ylabel('score')\n",
    "plt.title('Max Leaf Nodes')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The N Estimator plot seems very stable and hence resistant to overfitting.\n",
    "- Min Weight Fraction Leaf drops significantly after a certain point.\n",
    "- All other plots show very little variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Decision Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting dicision boundries with columns Age and Fare. \n",
    "from itertools import product\n",
    "\n",
    "#Picking Age and Fare as they are continuous and highly correlated with Survived\n",
    "X = df[['Age', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = GradientBoostingClassifier()\n",
    "\n",
    "#Fit models\n",
    "clf1.fit(X, y)\n",
    "clf2.fit(X, y)\n",
    "clf3.fit(X, y)\n",
    "\n",
    "# Plotting decision regions\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(1, 3, sharex='col', sharey='row', figsize=(15,4))\n",
    "\n",
    "for idx, clf, tt in zip(range(3),\n",
    "                        [clf1, clf2, clf3],\n",
    "                        ['Decision Tree', 'Random Forest',\n",
    "                         'Gradient Boost']):\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axarr[idx].contourf(xx, yy, Z, alpha=0.4)\n",
    "    axarr[idx].scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
    "    axarr[idx].set_title(tt, fontsize=14)\n",
    "    axarr[idx].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 3 plots are similar on the right side in terms of classification by initially splitting at Age. The second split occurs at the bottom for Fare. Discrepensies in classification occur as we move further than 2 splits. As more branches are forming, Decision Tree and Random Forest risk overfitting as they try to form more pure leaves. Relatively, Gradient Boost seem to be forming less branches and hence resistant to overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
